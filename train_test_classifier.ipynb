{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","metadata":{},"source":["must use: conda install xgboost<br>\n","import xgboost as xgb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"markdown","metadata":{},"source":["must use: conda install -c conda-forge category_encoders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from category_encoders import TargetEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import accuracy_score\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","from keras.utils.np_utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","import mlflow\n","import mlflow.sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from config import *\n","from data_preprocessing import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_rf_model(X_train, y_train):\n","    '''\n","        This function is to build a random forest model to use grid search. \n","    '''\n","    \n","    # Create a Random forest model \n","    clf = RandomForestClassifier()\n","    \n","    # Create the parameter grid based on the results of random search \n","    param_grid = {\n","        'max_depth': [20], # best 20, tried 10, 25, 30, 50, None -- good 20\n","        'n_estimators': [230] # best 200,210, 220, 230 tried 100, 120,235, 240, 250 -- good 200\n","    }\n","\n","    # Instantiate the grid search model\n","    grid_search = GridSearchCV(estimator = clf, \n","                               param_grid = param_grid, \n","                               cv = 10, \n","                               verbose = 3)\n","    \n","    # Fit the grid search to the data\n","    grid_search.fit(X_train, y_train)\n","    \n","    print('Best hyper-parameters:\\n', grid_search.best_params_)\n","\n","    # get the best model\n","    best_model = grid_search.best_estimator_\n","    \n","    save_model(best_model, './model/rf_model.pkl')\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_xgboost_model(X_train, y_train):\n","    '''\n","        This function is to build a xgboost model to use grid search. \n","    '''\n","    \n","    # Create a Random forest model \n","    clf = XGBClassifier(use_label_encoder=False)\n","    \n","    # Create the parameter grid based on the results of random search \n","    param_grid = {\n","        'max_depth': [6], # tried 20, default: 6\n","        'eval_metric': ['auc']\n","    }\n","\n","    # Instantiate the grid search model\n","    grid_search = GridSearchCV(estimator = clf, \n","                               param_grid = param_grid, \n","                               cv = 10, \n","                               verbose = 3)\n","    \n","    # Fit the grid search to the data\n","    y_train = [int(x) for x in y_train] # XGBoost does not support label encoded numbers\n","    grid_search.fit(X_train, y_train)\n","    \n","    print('Best hyper-parameters:\\n', grid_search.best_params_)\n","\n","    # get the best model\n","    best_model = grid_search.best_estimator_\n","    \n","    save_model(best_model, './model/xgboost_model.pkl')\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_deeplearning_model(X_train, y_train):\n","    '''\n","        This function is to build a MLP model with cross validation. \n","    '''\n","    \n","    # Create a Deep Learning model \n","    \n","    input_size = X_train.shape[1]\n","    \n","    if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:\n","        dl_model = Sequential()\n","        dl_model.add(Dense(64, input_dim = input_size, activation = 'relu'))\n","        dl_model.add(Dropout(0.5))\n","        dl_model.add(Dense(32, activation = 'relu'))\n","        dl_model.add(Dense(16, activation = 'relu'))\n","        dl_model.add(Dense(11, activation='softmax'))\n","    else:\n","        dl_model = Sequential()\n","        dl_model.add(Dense(32, input_dim = input_size, activation = 'relu'))\n","        dl_model.add(Dropout(0.5))\n","        dl_model.add(Dense(16, activation = 'relu'))\n","        dl_model.add(Dense(16, activation = 'relu'))\n","        dl_model.add(Dense(11, activation='softmax'))\n","    \n","    #\n","    # categorical_crossentropy can only accept one-hot encoded labels.\n","    # So need to convert label integers into numpy array of vectors, e.g., [[1. 0, 0, ..., 0],...]\n","    # See OneHotEncoding class for details.\n","    #\n","    # dl_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","    #\n","    # sparse_categorical_crossentropy allows integer labels\n","    #\n","    dl_model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","    \n","    dl_model.summary()\n","    \n","    # divide training data into two: one for training and one for validation\n","    X_train_data, X_eval_data, y_train_data, y_eval_data = \\\n","        train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n","    history = dl_model.fit(X_train_data, \n","                           y_train_data, \n","                           batch_size=15, \n","                           epochs=4, \n","                           validation_data=(X_eval_data, y_eval_data))\n","    \n","    if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:   \n","        save_model(dl_model, './model/dl_onehot_model.pkl')\n","    else:\n","        save_model(dl_model, './model/dl_target_encoding_model.pkl')\n","                     \n","    return dl_model, history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_rf_model(clf, feature_names, label_encoder, X_test, y_test, figsize=(15,15)):\n","    '''\n","        This function is doing the following:\n","        1. predict and caculate the accuracy score.\n","        2. draw confusion matrix diagram.\n","        3. find and display feature importance figure.\n","        \n","    '''\n","    y_predict = clf.predict(X_test)\n","    \n","    acc_score = accuracy_score(y_test, y_predict)\n","    \n","    print('Accuracy score: ', acc_score)\n","    # this function does the same thing as above the two function calls.\n","    # clf.score(X_test, y_test) \n","    \n","    label_values = label_encoder.inverse_transform(clf.classes_)\n","    cm = confusion_matrix(y_test, y_predict, labels=clf.classes_)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values) # clf.classes_)\n","    fig, ax = plt.subplots(figsize=figsize)\n","    disp.plot(ax=ax)\n","    \n","    importances = clf.feature_importances_\n","    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n","    # feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n","    # feature_names = train_data_target_encoding.columns.tolist()\n","    forest_importances = pd.Series(importances, index=feature_names)\n","    fig, ax = plt.subplots()\n","    forest_importances.plot.bar(yerr=std, ax=ax)\n","    ax.set_title(\"Feature importances using MDI\")\n","    ax.set_ylabel(\"Mean decrease in impurity\")\n","    fig.tight_layout()\n","    \n","    return acc_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_xgboost_model(clf, feature_names, label_encoder, X_test, y_test):\n","    '''\n","        This function is doing the following:\n","        1. pridict and caculate the accuracy score.\n","        2. draw confusion matrix diagram.\n","        3. find and display feature importance figure.   \n","    '''\n","    y_predict = clf.predict(X_test)\n","    \n","    acc_score = accuracy_score(y_test, y_predict)\n","    print('Accuracy score: ', acc_score)\n","    # this function does the same thing as above the two function calls.\n","    # clf.score(X_test, y_test) \n","    \n","    label_values = label_encoder.inverse_transform(clf.classes_)\n","    cm = confusion_matrix(y_test, y_predict, labels=clf.classes_)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_values) # clf.classes_)\n","    fig, ax = plt.subplots(figsize=(15, 15))\n","    disp.plot(ax=ax)\n","    \n","    return acc_score "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_dl_model(dl_model, history, label_encoder, X_test, y_test):\n","    '''\n","        This function is doing the following:\n","        1. pridict and caculate the accuracy score.\n","        2. draw confusion matrix diagram.\n","        3. find and display feature importance figure.\n","        \n","    '''\n","    y_pred = dl_model.predict(X_test)\n","    # convert deep learning model (MLP) output vector [a0, a1, ..., a10] to the index with the max value\n","    # y_pred_labels return indices, each sample (row) has a index, like 0, 1, ..., 10\n","    y_pred_labels = np.apply_along_axis(np.argmax, 1, y_pred) \n","    # convert above indices 0, 1, ..., 10 back into ranges, like \"21-30\", ...\n","    # label_values = label_encoder.inverse_transform(y_pred_labels)\n","    test_loss, test_accuracy = dl_model.evaluate(X_test, y_test)\n","    print('loss: ', test_loss, 'accuracy:', test_accuracy)\n","\n","    # Ploting the traning and valitation loss  \n","    \n","    history_dict = history.history\n","    loss_values = history_dict['loss']\n","    val_loss_values = history_dict['val_loss']\n","    epochs = range(1, len(loss_values) + 1)\n","    \n","    plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","    plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","    \n","    plt.clf()\n","    acc_values = history_dict['accuracy']\n","    val_acc_values = history_dict['val_accuracy']\n","    \n","    plt.plot(epochs, acc_values, 'bo',label='Training acc')\n","    plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    \n","    # confusion matrix\n","    label_classes = list(range(11))\n","    label_classes_range = label_encoder.inverse_transform(label_classes)\n","    # labels=label_classes: 0, 1, .., 10\n","    cm = confusion_matrix(y_test, y_pred_labels, labels=label_classes)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_classes_range) # clf.classes_)\n","    fig, ax = plt.subplots(figsize=(15, 15))\n","    disp.plot(ax=ax)\n","    \n","    return test_loss, test_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_model(model, model_filepath):\n","    '''\n","       Save trained model into pickle file.\n","    '''\n","    # save the model to disk\n","    pickle.dump(model, open(model_filepath, 'wb'))\n","    \n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model():\n","    '''\n","      load model from pickle file.\n","    '''\n","    # load the model from disk\n","    if USE_RANDOM_FOREST:\n","        loaded_model = pickle.load(open('./model/rf_model.pkl', 'rb'))\n","    elif USE_XGBOOST:\n","        loaded_model = pickle.load(open('./model/xgboost_model.pkl', 'rb'))\n","    elif USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:  \n","        loaded_model = pickle.load(open('./model/dl_onehot_model.pkl', 'rb'))\n","    else: # USE_DEEP_LEARNING_WITH_TARGET_ENCODING\n","        loaded_model = pickle.load(open('./model/dl_target_encoding_model.pkl', 'rb'))\n","        \n","    return loaded_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def mlFlow():\n","    '''\n","    Training model in mlflow\n","    '''\n","    np.random.seed(42)\n","    \n","    target_encoders = None\n","    label_encoder = None\n","    \n","    experiment_name = 'Predicting-Length-OfStay-Of-Patient-In-a-Hospital'\n","            \n","    mlflow.set_experiment(experiment_name)\n","    \n","    if USE_RANDOM_FOREST:\n","        run_name = 'Random Forest'\n","    elif USE_XGBOOST:\n","        run_name = 'XGBoost'\n","    elif USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n","        run_name = 'MLP with Target Encoding'\n","    else:\n","        run_name = 'MLP with One-Hot Encoding'\n","    \n","    with mlflow.start_run(run_name=run_name):\n","\n","        #\n","        # Load training data\n","        #\n","        print('\\nLOAD TRAINING DATA: train_data.csv\\n')\n","        train_data = load_data('train_data.csv')\n","        \n","        #\n","        # Data Preprocessing\n","        #\n","        print('\\nDATA PROCESSING: \\n')\n","        if USE_RANDOM_FOREST or USE_XGBOOST or USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n","            # data preprocessing using target encoding\n","            X_train, X_test, y_train, y_test, feature_names, target_encoders, label_encoder = \\\n","            target_encoding_preprocessing(train_data, label_encoding_target=True)\n","        if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:\n","            # data preprocessing using onehot encoding\n","            X_train, X_test, y_train, y_test, feature_names, label_encoder = \\\n","                onehot_encoding_preprocessing(train_data)\n","    \n","        #\n","        # Model training\n","        #\n","        print('\\nMODEL TRAINING: \\n')\n","        if USE_RANDOM_FOREST:\n","            # Build random forest model using GridSearch\n","            mlflow.log_param('model', 'Random Forest')\n","            mlflow.log_param('n_estimators', 230)\n","            mlflow.log_param('max_depth', 20)\n","            mlflow.log_param('data preprocessing method', 'target_encoding_preprocessing')\n","            mlflow.log_param('label encoding method', 'LabelEncoder')\n","            \n","            clf = build_rf_model(X_train, y_train)\n","            \n","        if USE_XGBOOST:\n","            # Build xgboost model using GridSearch\n","            mlflow.log_param('model', 'XGBoost')\n","            mlflow.log_param('max_depth', 6)\n","            mlflow.log_param('eval_metric', 'auc')\n","            mlflow.log_param('data preprocessing method', 'target_encoding_preprocessing')\n","            mlflow.log_param('label encoding method', 'LabelEncoder')\n","            \n","            clf = build_xgboost_model(X_train, y_train)\n","            \n","        if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:\n","            mlflow.log_param('model', 'Deep Learning - MLP')\n","            mlflow.log_param('MLP layers', '64, dropout 0.5, 32, 16, 11')\n","            mlflow.log_param('loss', 'sparse_categorical_crossentropy')\n","            mlflow.log_param('optimizer', 'rmsprop')\n","            mlflow.log_param('metrics', 'accuracy')\n","            mlflow.log_param('data preprocessing method', 'onehot_encoding_preprocessing')\n","            mlflow.log_param('label encoding method', 'LabelEncoder')\n","            \n","            clf, history = build_deeplearning_model(X_train, y_train)\n","            \n","        if USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n","            mlflow.log_param('model', 'Deep Learning - MLP')\n","            mlflow.log_param('MLP layers', '32, dropout 0.5, 16, 16, 11')\n","            mlflow.log_param('loss', 'sparse_categorical_crossentropy')\n","            mlflow.log_param('optimizer', 'rmsprop')\n","            mlflow.log_param('metrics', 'accuracy')\n","            mlflow.log_param('data preprocessing method', 'target_encoding_preprocessing')\n","            mlflow.log_param('label encoding method', 'LabelEncoder')\n","            \n","            clf, history = build_deeplearning_model(X_train, y_train)\n","        \n","        #\n","        # Model evaluation\n","        #\n","        print('\\nMODEL EVALUATION: \\n')\n","        if USE_RANDOM_FOREST:\n","            acc_score = evaluate_rf_model(clf, feature_names, label_encoder, X_test, y_test, figsize=(10,10))\n","            mlflow.log_metric('accuracy', acc_score)\n","            mlflow.sklearn.log_model(clf, \"random_forest_model\")\n","        if USE_XGBOOST:\n","            acc_score = evaluate_xgboost_model(clf, feature_names, label_encoder, X_test, y_test)\n","            mlflow.log_metric('accuracy', acc_score)\n","            mlflow.sklearn.log_model(clf, \"xgboost_model\")\n","        if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING or USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n","            test_loss, test_accuracy = evaluate_dl_model(clf, history, label_encoder, X_test, y_test)\n","            mlflow.log_metric(\"loss\", test_loss)\n","            mlflow.log_metric(\"accuracy\", test_accuracy)\n","            mlflow.sklearn.log_model(clf, \"deep_learning_model\")\n","                \n","        print('\\n ML FLOW DONE. \\n')\n","    return target_encoders, label_encoder\n"," \n","def predict(label_encoder, target_encoders, test_data_file='test_data.csv'):\n","    '''\n","    predict results from test dataset in deployment\n","    '''\n","    test_data = load_data(test_data_file)\n","    \n","    if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:\n","        # Data preprocessing for prediction\n","        transformed_features = onehot_encoding_preprocessing_for_prediction(test_data)\n","    else:\n","        transformed_features = target_encoding_preprocessing_for_prediction(test_data, target_encoders)\n","\n","    # load the model from disk\n","    loaded_model = load_model()\n","    \n","    # Predict the \"Days of Stay\" for patients.\n","    predicted_labels = loaded_model.predict(transformed_features)\n","    if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING or USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n","        # convert deep learning model (MLP) output vector [a0, a1, ..., a10] to the index with the max value\n","        predicted_labels = np.apply_along_axis(np.argmax, 1, predicted_labels)\n","\n","    # convert sequential labeling number 0, 1, ..., 10 to the original label like '21-30'\n","    predict_range = label_encoder.inverse_transform(predicted_labels)\n","\n","    # Show original testing data with predicted results.\n","    result_df = test_data.copy()\n","    result_df['Stay'] = predict_range\n","    \n","    return result_df"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
