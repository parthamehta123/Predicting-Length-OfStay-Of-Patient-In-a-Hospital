{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.ensemble import RandomForestClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["must use: conda install xgboost<br>\n", "import xgboost as xgb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from xgboost import XGBClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["must use: conda install -c conda-forge category_encoders"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from category_encoders import TargetEncoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import roc_auc_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import keras\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Activation\n", "from keras.utils.np_utils import to_categorical"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle\n", "import mlflow\n", "import mlflow.sklearn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from config import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(file_name, filepath='./data/'):\n", "    '''\n", "    This function is to load csv file as Pandas dataframe.\n", "    '''\n", "    filepath = filepath + '/' + file_name\n", "    data = pd.read_csv(filepath)\n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_average(value_range):\n", "    '''\n", "    This function is to convert an age range [x, y] to an average age of (x+y)/2.\n", "    '''\n", "    if value_range in ['More than 100 Days']:\n", "        return 105\n", "    \n", "    alist = value_range.split('-')\n", "    average_value = (float(alist[0]) + float(alist[1]))/2\n", "    average_value = int(round(average_value, 0))\n", "    \n", "    return average_value"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_encoders(label_encoder, target_encoders):\n", "    '''\n", "       Save trained label encoder and target encoders (one per categorical feature) into pickle files.\n", "    '''\n", "    \n", "    # save the label encoder into file\n", "    if label_encoder is not None:\n", "        pickle.dump(label_encoder, open('./model/label_encoder.pkl', 'wb'))   \n", "    \n", "    # save the target encoders into file\n", "    if target_encoders is not None:\n", "        pickle.dump(target_encoders, open('./model/target_encoders.pkl', 'wb'))  \n", "    \n", "    return"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_encoders():\n", "    '''\n", "       Load trained label encoder and target encoders (one per categorical feature) from pickle files.\n", "    '''\n", "    label_encoder = None\n", "    target_encoders = None\n", "    \n", "    try:\n", "        # load the label encoder from file\n", "        label_encoder = pickle.load(open('./model/label_encoder.pkl', 'rb'))\n\n", "        # load the target encoders from file\n", "        target_encoders = pickle.load(open('./model/target_encoders.pkl', 'rb'))\n", "    except:\n", "        print('load_encoders(): target_encoders.pkl does not exist for model with one-hot encoding!')\n", "    \n", "    return label_encoder, target_encoders"]}, {"cell_type": "markdown", "metadata": {}, "source": ["inheriting BaseEstimator, TransformerMixin to utilize pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataCleaning(BaseEstimator, TransformerMixin):\n", "    '''\n", "    this class is doing the following:\n", "        1. drop missing data because the total number of missing data is small.\n", "        2. \n", "    inheriting BaseEstimator, TransformerMixin to utilize pipeline     \n", "    '''\n", "    def __init__(self, for_prediction = False):\n", "        self.for_prediction = for_prediction\n\n", "    # inherited from BaseEstimator\n", "    def fit(self, x, y=None):\n", "        print('DataCleaning.fit ...')\n", "        return self\n", "    \n", "    # inherited from TransformerMixin\n", "    def transform(self, X):\n", "        print('DataCleaning.transform ...')\n", "        # drop rows with missing data\n", "        if not isinstance(X, pd.DataFrame):\n", "            print('DataCleaning.transform: Error - X is not a dataframe')\n", "            return X\n", "            \n", "        X1 = X.copy()\n", "        \n", "        # if not self.for_prediction:\n", "        #    X1 = X1.dropna(axis=0)\n", "        X1 = X1.fillna(0)\n", "        \n", "        # drop columns without prediction power\n", "        drop_columns = ['case_id'] \n", "        for col in drop_columns: \n", "            if col in X1.columns:\n", "                X1.drop([col], axis=1, inplace=True)\n", "        \n", "        return X1\n", "        \n", "class TargetEncoding(BaseEstimator, TransformerMixin):\n", "    '''\n", "    this class performs target encoding:\n", "        1. convert categorical targets/labels into numbers.\n", "        2. convert categorical features into numbers.\n", "        \n", "    inheriting BaseEstimator, TransformerMixin to utilize pipeline   \n", "    '''\n", "    def __init__(self, label_encoding_target, label_name):\n", "        '''\n", "        label_encoding_target: True - use label encoder to convert label column 'Stay'\n", "                               False - use get_average() function to convert\n", "        label_name: label column name ('Stay')\n", "        '''\n", "        self.label_encoding_target = label_encoding_target\n", "        self.label_name = label_name\n", "        self.label_encoder = None\n", "        self.target_encoders = dict()\n", "    def fit(self, x, y=None):\n", "        print('TargetEncoding.fit ...')\n", "        return self\n", "    def transform(self, X):\n", "        '''\n", "        Perform target encoding transformation\n", "        '''\n", "        print('TargetEncoding.transform ...')\n", "        if not isinstance(X, pd.DataFrame):\n", "            print('TargetEncoding.transform: Error - X is not a dataframe')\n", "            return X\n", "        \n", "        X1 = X.copy()\n", "        \n", "        # Step 1: get categorical feature names\n", "        categorical_columns = []\n", "        for idx, col in enumerate(X1.columns):\n", "            # dtypes identify datatype of each column. idx identify each column.\n", "            if X1.dtypes[idx] == object:\n", "                if col != self.label_name:\n", "                    categorical_columns.append(col)\n", "                    \n", "        # Step 2: convert the label column into numbers required by Target Encording.\n", "        # two options are provided below.\n", "        if self.label_encoding_target:           \n", "            self.label_encoder = LabelEncoder()\n", "            X1.loc[:,self.label_name] = self.label_encoder.fit_transform(X1[self.label_name])\n", "            \n", "            # save label encoder into file\n", "            save_encoders(self.label_encoder, None)\n", "        else:\n", "            X1.loc[:, self.label_name] = X1[self.label_name].apply(lambda x: get_average(x))\n\n", "        # Step 3: convert the values of categorical features in each column into numbers by taking \n", "        # the average of corresponding aggregated label values:\n", "        for col in categorical_columns:\n", "            # must create a new target encode object (TargetEncoder) for each categorical feature.\n", "            encoder = TargetEncoder() \n", "            # X1.loc[:, col] = encoder.fit_transform(X1[col], X1[self.label_name])\n", "            encoder.fit(X1[col], X1[self.label_name])\n", "            X1.loc[:, col] = encoder.transform(X1[col])\n", "            self.target_encoders[col] = encoder # save trained target encoders for deployment\n", "            \n", "        # save target eocoders into file\n", "        save_encoders(None, self.target_encoders)\n", "        return X1\n", "        \n", "class TargetEncodingForPrediction(BaseEstimator, TransformerMixin):\n", "    '''\n", "    this class performs target encoding:\n", "        1. convert categorical features into numbers in deployment.\n", "        \n", "    inheriting BaseEstimator, TransformerMixin to utilize pipeline   \n", "    '''\n", "    def __init__(self, target_encoders):\n", "        '''\n", "        target_encoders: This is a dictionary of target encoders:\n", "                         categorical feature name 'Age' --> trained target encoder for 'Age' column\n", "        '''\n", "        self.target_encoders = target_encoders\n", "    def fit(self, x, y=None):\n", "        print('TargetEncodingForPrediction.fit ...')\n", "        return self\n", "    def transform(self, X):\n", "        print('TargetEncodingForPrediction.transform ...')\n", "        if not isinstance(X, pd.DataFrame):\n", "            print('TargetEncodingForPrediction.transform: Error - X is not a dataframe')\n", "            return X\n", "        \n", "        X1 = X.copy()\n", "        \n", "        # Step 1: get categorical feature names\n", "        categorical_columns = []\n", "        for idx, col in enumerate(X1.columns):\n", "            # dtypes identify datatype of each column. idx identify each column.\n", "            if X1.dtypes[idx] == object:\n", "                categorical_columns.append(col)\n", "                    \n", "        # Step 2: convert the values of categorical features in each column into numbers by taking \n", "        # the average of corresponding aggregated label values:\n", "        for col in categorical_columns:\n", "            # find the target encode object for this categorical column.\n", "            encoder = self.target_encoders[col]\n", "            X1.loc[:, col] = encoder.transform(X1[col])\n", "        return X1\n", "        \n", "class OneHotEncoding(BaseEstimator, TransformerMixin):\n", "    '''\n", "    this class performs one-hot encoding:\n", "        1. convert categorical targets/labels into numbers.\n", "        2. convert categorical features into numbers.\n", "        \n", "    inheriting BaseEstimator, TransformerMixin to utilize pipeline \n", "    '''\n", "    def __init__(self, label_name, for_prediction=False):\n", "        '''\n", "        label_name: label column name ('Stay')\n", "        '''\n", "        self.label_name = label_name\n", "        self.label_encoder = None\n", "        self.for_prediction = for_prediction\n", "    def fit(self, x, y=None):\n", "        print('OneHotEncoding.fit ...')\n", "        return self\n", "    def transform(self, X):\n", "        print('OneHotEncoding.transform ...')\n", "        if not isinstance(X, pd.DataFrame):\n", "            print('OneHotEncoding.transform: Error - X is not a dataframe')\n", "            return X\n", "        \n", "        X1 = X.copy()\n", "                    \n", "        # Step 1: convert the label column into numbers required by deep learning.\n", "        # This step must be done first to avoid one-hot encoding this label column \n", "        # by get_dummies().\n", "        if not self.for_prediction:\n", "            self.label_encoder = LabelEncoder() # save LabelEncoder for later reverse conversion.\n", "            X1.loc[:,self.label_name] = self.label_encoder.fit_transform(X1[self.label_name])\n", "            # convert label_dl integers into numpy array of vectors, e.g., [[1. 0, 0, ..., 0],...]\n", "            # label_dl = to_categorical(label_dl)\n", "            \n", "            # save label encoder into file\n", "            save_encoders(self.label_encoder, None)\n", "      \n", "        # Step 2 (Improvement): convert Age range column into numbers\n", "        X1.loc[:,'Age'] = X1['Age'].apply(lambda x: get_average(x))\n\n", "        # Step 3: one-hot encode the values of categorical features in X1\n", "        X1 = pd.get_dummies(X1)\n", "        return X1\n", "        \n", "class FeatureNormorlization(BaseEstimator, TransformerMixin):\n", "    '''\n", "    this class performs one-hot encoding:\n", "        1. convert categorical targets/labels into numbers.\n", "        2. convert categorical features into numbers.\n", "        \n", "    inheriting BaseEstimator, TransformerMixin to utilize pipeline \n", "    '''\n", "    def __init__(self):\n", "        pass\n", "    def fit(self, x, y=None):\n", "        print('FeatureNormorlization.fit ...')\n", "        return self\n", "    def transform(self, X):\n", "        print('FeatureNormorlization.transform ...')\n", "        if not isinstance(X, pd.DataFrame):\n", "            print('FeatureNormorlization.transform: Error - X is not a dataframe')\n", "            return X\n", "        \n", "        X1 = X.copy()\n", "                    \n", "        # normalize feature's conponent for deep learning.\n", "        # normalize numeric columns\n", "        for idx, col in enumerate(X1.columns):\n", "             if X1.dtypes[idx] != object and col != 'Stay':\n", "                col_data = X1[col]\n", "                mean = col_data.mean()\n", "                col_data -= mean\n", "                std = col_data.std()\n", "                col_data /= std\n", "                X1.loc[:, col] = col_data\n", "        return X1\n", "        \n", "def target_encoding_preprocessing(train_data, label_encoding_target, label_name='Stay'):\n", "    '''\n", "        label_encoding_target: True - use label encoder to conver label column\n", "    \n", "        This process performs two things and is used for Random Forest Model:\n", "        1. data cleaning.\n", "        2. target encoding.\n", "    '''\n", "    target_encoder = TargetEncoding(label_encoding_target, label_name)\n", "    \n", "    if USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n", "        target_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning()),\n", "            ('target_encoding', target_encoder),\n", "            ('feature_normorlization', FeatureNormorlization())\n", "            ])        \n", "    else:\n", "        target_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning()),\n", "            ('target_encoding', target_encoder)\n", "            ])\n", "    \n", "    train_data_transformed = target_encoding_pipeline.fit_transform(train_data)\n", "    \n", "    # Find the correlations between features and labels\n", "    '''\n", "    Hospital_type_code                   0.082574\n", "    City_Code_Hospital                   0.006626\n", "    Hospital_region_code                 0.012746\n", "    Available Extra Rooms in Hospital   -0.121564\n", "    Department                           0.037188\n", "    Ward_Type                            0.193062\n", "    Ward_Facility_Code                   0.084282\n", "    Bed Grade                            0.025056\n", "    patientid                            0.000932\n", "    City_Code_Patient                   -0.009779\n", "    Type of Admission                    0.092835\n", "    Severity of Illness                  0.126883\n", "    Visitors with Patient                0.537476\n", "    Age                                  0.097167\n", "    Admission_Deposit                   -0.052092\n", "    Stay                                 1.000000\n", "    '''\n", "    print('\\nFind the correlations between each feature and label (Stay):\\n')\n", "    print(train_data_transformed[train_data_transformed.columns[0:]].corr()['Stay'][:])\n", "       \n", "    # Shuffle dataset to prevent artifical data patterns\n", "    train_data_transformed = train_data_transformed.sample(frac=1)\n", "    \n", "    # Separate label column from feature columns\n", "    y = train_data_transformed[label_name].values # convert Pandas Series into numpy array\n", "    train_data_transformed.drop([label_name], axis=1, inplace=True)\n", "    X = train_data_transformed.values # convert Pandas dataframe into numpy array\n", "    feature_names = train_data_transformed.columns # it is needed to check feature importance.\n\n", "    # Split dataset into training and testing subsets.\n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "    \n", "    return X_train, X_test, y_train, y_test, feature_names, target_encoder.target_encoders, target_encoder.label_encoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def target_encoding_preprocessing_for_prediction(test_data, target_encoders):\n", "    '''\n", "        test_data: dataframe without label column.\n", "        \n", "        This process performs two things and is used for deployment (prediction).\n", "        1. data cleaning.\n", "        2. target encoding.\n", "    '''\n", "    target_encoder = TargetEncodingForPrediction(target_encoders)\n", "    \n", "    if USE_DEEP_LEARNING_WITH_TARGET_ENCODING:\n", "        target_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning(for_prediction=True)),\n", "            ('target_encoding', target_encoder),\n", "            ('feature_normorlization', FeatureNormorlization())\n", "            ])        \n", "    else:\n", "        target_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning(for_prediction=True)),\n", "            ('target_encoding', target_encoder)\n", "            ])\n", "    \n", "    test_data_transformed = target_encoding_pipeline.fit_transform(test_data)\n", "       \n", "    X = test_data_transformed.values # convert Pandas dataframe into numpy array of features\n", "    \n", "    return X"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def onehot_encoding_preprocessing(train_data, label_name='Stay'):\n", "    '''\n", "        This process performs two things and is used for Deep Learning Model:\n", "        1. data cleaning.\n", "        2. target encoding.\n", "        \n", "    '''\n", "    # Create OneHotEncoding object outside of the Pipeline \n", "    # for convenience in getting label_encoder from the object.\n", "    onehot_encoder = OneHotEncoding(label_name)\n", "    onehot_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning()),\n", "            ('feature_normorlization', FeatureNormorlization()),\n", "            ('onehot_encoding', onehot_encoder)\n", "            ])\n", "    \n", "    train_data_transformed = onehot_encoding_pipeline.fit_transform(train_data)\n", "       \n", "    # Shuffle dataset to prevent artifical data patterns\n", "    train_data_transformed = train_data_transformed.sample(frac=1)\n", "    \n", "    # Separate label column from feature columns\n", "    y = train_data_transformed[label_name].values # convert Pandas Series into numpy array\n", "    train_data_transformed.drop([label_name], axis=1, inplace=True)\n", "    X = train_data_transformed.values # convert Pandas dataframe into numpy array\n", "    # The featue names can be used to check feature importance for Random Forest model.\n", "    feature_names = train_data_transformed.columns \n\n", "    # Split dataset into training and testing subsets.\n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "    \n", "    return X_train, X_test, y_train, y_test, feature_names, onehot_encoder.label_encoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def onehot_encoding_preprocessing_for_prediction(test_data):\n", "    '''\n", "        test_data: dataframe without label column.\n", "        \n", "        This process performs two things and is used for deployment (prediction).\n", "        1. data cleaning.\n", "        2. target encoding.\n", "    '''\n", "    \n", "    if USE_DEEP_LEARNING_WITH_ONEHOT_ENCODING:\n", "        onehot_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning(for_prediction=True)),\n", "            ('feature_normorlization', FeatureNormorlization()),\n", "            ('onehot_encoding', OneHotEncoding(label_name=None, for_prediction=True))\n", "            ])        \n", "    else:\n", "        onehot_encoding_pipeline = Pipeline([\n", "            ('data_cleaning', DataCleaning(for_prediction=True)),\n", "            ('onehot_encoding', OneHotEncoding(label_name=None, for_prediction=True))\n", "            ])\n", "    \n", "    test_data_transformed = onehot_encoding_pipeline.fit_transform(test_data)\n", "       \n", "    X = test_data_transformed.values # convert Pandas dataframe into numpy array of features\n", "    \n", "    return X"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}